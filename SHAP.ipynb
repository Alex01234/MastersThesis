{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Alexander Dolk and Hjalmar Davidsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('<PATH TO TOKENIZER>', \n",
    "                                                   local_files_only=True,\n",
    "                                                   model_max_length=512,\n",
    "                                                   max_len=512,\n",
    "                                                   truncation=True,\n",
    "                                                   padding='Longest')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('<PATH TO CLASSIFICATION MODEL>', local_files_only=True, problem_type=\"multi_label_classification\", num_labels=18)\n",
    "pred = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "labels = ['K567', 'K573', 'K358', 'K590', 'K800', 'K379', 'K802', 'K610', 'K566', 'K509', 'K859', 'K572', 'K353', 'K650', 'K922', 'K565', 'K210', 'K560']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this defines an explicit python function that takes a list of strings and outputs scores for each class\n",
    "def f(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128, truncation=True) for v in x])\n",
    "    attention_mask = (tv!=0).type(torch.int64)\n",
    "    outputs = model(tv,attention_mask=attention_mask)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores)\n",
    "    return val\n",
    "\n",
    "method = \"custom tokenizer\"\n",
    "\n",
    "# build an explainer by passing a transformers tokenizer\n",
    "if method == \"transformers tokenizer\":\n",
    "    explainer = shap.Explainer(f, tokenizer, output_names=labels)\n",
    "\n",
    "# build an explainer by explicitly creating a masker\n",
    "elif method == \"default masker\":\n",
    "    masker = shap.maskers.Text(r\"\\W\") # this will create a basic whitespace tokenizer\n",
    "    explainer = shap.Explainer(f, masker, output_names=labels)\n",
    "\n",
    "# build a fully custom tokenizer\n",
    "elif method == \"custom tokenizer\":\n",
    "    import re\n",
    "\n",
    "    def custom_tokenizer(s, return_offsets_mapping=True):\n",
    "        \"\"\" Custom tokenizers conform to a subset of the transformers API.\n",
    "        \"\"\"\n",
    "        pos = 0\n",
    "        offset_ranges = []\n",
    "        input_ids = []\n",
    "        for m in re.finditer(r\"\\W\", s):\n",
    "            start, end = m.span(0)\n",
    "            offset_ranges.append((pos, start))\n",
    "            input_ids.append(s[pos:start])\n",
    "            pos = end\n",
    "        if pos != len(s):\n",
    "            offset_ranges.append((pos, len(s)))\n",
    "            input_ids.append(s[pos:])\n",
    "        out = {}\n",
    "        out[\"input_ids\"] = input_ids\n",
    "        if return_offsets_mapping:\n",
    "            out[\"offset_mapping\"] = offset_ranges\n",
    "        return out\n",
    "\n",
    "    masker = shap.maskers.Text(custom_tokenizer)\n",
    "    explainer = shap.Explainer(f, masker, output_names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_values = explainer([\"<DISCHARGE SUMMARY TO EXPLAIN>\"])\n",
    "shap.plots.text(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
